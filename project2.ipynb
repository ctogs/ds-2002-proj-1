{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DS-2002 — Data Project 2 (Capstone)\n",
        "\n",
        "## Project Overview (Local PySpark Lakehouse)\n",
        "\n",
        "This notebook implements the Project 2 capstone **locally** using **PySpark** and implements the “Bronze → Silver → Gold” lakehouse architecture with both **batch** and **streaming** ingestion.\n",
        "\n",
        "### Business process modeled\n",
        "- **Retail sales transactions** (Fact table grain: one row per order line)\n",
        "\n",
        "### Dimensional model (Gold)\n",
        "- **Dimensions**:\n",
        "  - `dim_date` (from local CSV)\n",
        "  - `dim_customer` (from MySQL `src_customers`)\n",
        "  - `dim_product` (from MySQL `src_products`)\n",
        "  - `dim_category` (derived from product attributes)\n",
        "- **Facts**:\n",
        "  - `fact_sales` (streaming-conformed fact)\n",
        "  - `fact_sales_margin` (gross margin metric derived using MongoDB costs)\n",
        "\n",
        "### Required source systems (explicitly demonstrated)\n",
        "- **Relational (SQL)**: **MySQL** accessed via **Spark JDBC** (`src_customers`, `src_products`)\n",
        "- **NoSQL**: **MongoDB (local or Atlas)** accessed via `pymongo` (`product_costs`)\n",
        "- **File system**: local CSVs for dates and sales; local JSON files for streaming intervals\n",
        "\n",
        "### Required processing behaviors (local equivalents)\n",
        "- **Batch execution + incremental load**: initial batch load plus an explicit incremental batch append (row counts increase)\n",
        "- **Streaming mini-batches (3 intervals)**: sales fact data is segmented into **3 JSON drops** and ingested using **Structured Streaming** (local equivalent of Databricks AutoLoader)\n",
        "- **Silver integration**: streaming facts are joined to static dimensions in the **Silver** layer prior to publishing Gold tables\n",
        "\n",
        "### Outputs\n",
        "- Lakehouse tables are written under `project2_lakehouse/{bronze,silver,gold}/`.\n",
        "- Streaming inputs/checkpoints are under `project2_stream/`.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "import certifi\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path.cwd()\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "\n",
        "LAKEHOUSE_DIR = BASE_DIR / \"project2_lakehouse\"\n",
        "BRONZE_DIR = LAKEHOUSE_DIR / \"bronze\"\n",
        "SILVER_DIR = LAKEHOUSE_DIR / \"silver\"\n",
        "GOLD_DIR   = LAKEHOUSE_DIR / \"gold\"\n",
        "\n",
        "STREAM_DIR = BASE_DIR / \"project2_stream\"\n",
        "INCOMING_DIR = STREAM_DIR / \"incoming\"\n",
        "CHECKPOINT_DIR = STREAM_DIR / \"checkpoints\" / \"bronze_sales\"\n",
        "\n",
        "# Source files reused from Project 1\n",
        "CUSTOMERS_CSV = DATA_DIR / \"customers.csv\"\n",
        "PRODUCTS_CSV  = DATA_DIR / \"products.csv\"\n",
        "DATES_CSV     = DATA_DIR / \"dates.csv\"\n",
        "SALES_CSV     = DATA_DIR / \"sales.csv\"\n",
        "PRODUCT_COSTS_JSON = DATA_DIR / \"product_costs.json\"\n",
        "\n",
        "USE_MYSQL = True\n",
        "USE_MONGO = True\n",
        "\n",
        "STRICT_SOURCES = True\n",
        "\n",
        "MYSQL = {\n",
        "    \"host\": \"localhost\",\n",
        "    \"port\": 3306,\n",
        "    \"database\": \"northwind_dw2\",\n",
        "    \"user\": \"root\",\n",
        "    \"password\": \"thevidu\",\n",
        "    \"customers_table\": \"src_customers\",\n",
        "    \"products_table\": \"src_products\",\n",
        "}\n",
        "\n",
        "\n",
        "MONGO = {\n",
        "    \"uri\": \"mongodb://localhost:27017\",\n",
        "    \"database\": \"northwind_purchasing\",\n",
        "    \"collection\": \"product_costs\",\n",
        "}\n",
        "\n",
        "JDBC_URL = f\"jdbc:mysql://{MYSQL['host']}:{MYSQL['port']}/{MYSQL['database']}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Start Spark (local)\n",
        "\n",
        "This uses a local Spark session. Storage format is **Parquet** to keep setup simple and aligned with Lab 6.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using spark.jars.packages for MySQL JDBC: com.mysql:mysql-connector-j:9.1.0\n",
            ":: loading settings :: url = jar:file:/Users/collintogher/Documents/ds-2002-proj-1/.venv311/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /Users/collintogher/.ivy2/cache\n",
            "The jars for the packages stored in: /Users/collintogher/.ivy2/jars\n",
            "com.mysql#mysql-connector-j added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-64079151-1937-43bc-ad06-ef35d4126f18;1.0\n",
            "\tconfs: [default]\n",
            "\tfound com.mysql#mysql-connector-j;9.1.0 in central\n",
            "\tfound com.google.protobuf#protobuf-java;4.26.1 in central\n",
            ":: resolution report :: resolve 82ms :: artifacts dl 2ms\n",
            "\t:: modules in use:\n",
            "\tcom.google.protobuf#protobuf-java;4.26.1 from central in [default]\n",
            "\tcom.mysql#mysql-connector-j;9.1.0 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-64079151-1937-43bc-ad06-ef35d4126f18\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
            "25/12/19 21:25:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 3.5.4\n",
            "java: /usr/bin/java\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    spark\n",
        "    spark.stop()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    from pyspark import SparkContext\n",
        "\n",
        "    if SparkContext._active_spark_context is not None:\n",
        "        SparkContext._active_spark_context.stop()\n",
        "\n",
        "    # Clear stale gateway references so getOrCreate can launch a fresh JVM\n",
        "    SparkContext._active_spark_context = None\n",
        "    SparkContext._gateway = None\n",
        "    SparkContext._jvm = None\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "builder = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"DS-2002-Project-2-Local-PySpark\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.sql.warehouse.dir\", str((BASE_DIR / \"spark-warehouse\").resolve()))\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
        ")\n",
        "\n",
        "builder = builder.config(\"spark.jars.packages\", \"com.mysql:mysql-connector-j:9.1.0\")\n",
        "print(\"Using spark.jars.packages for MySQL JDBC: com.mysql:mysql-connector-j:9.1.0\")\n",
        "\n",
        "spark = builder.getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"Spark version:\", spark.version)\n",
        "print(\"java:\", java_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1b) Source system setup (MySQL + MongoDB)\n",
        "\n",
        "Reads from:\n",
        "- **MySQL** (SQL) for `src_customers` and `src_products`\n",
        "- **MongoDB** (NoSQL) for `product_costs`\n",
        "- **Local CSV files** for date and fact data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded MySQL tables: src_customers , src_products\n"
          ]
        }
      ],
      "source": [
        "mysql_props = {\n",
        "    \"user\": MYSQL[\"user\"],\n",
        "    \"password\": MYSQL[\"password\"],\n",
        "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
        "}\n",
        "\n",
        "customers_seed = [\n",
        "    {\"customer_id\": \"CUST-001\", \"first_name\": \"Ava\", \"last_name\": \"Nguyen\", \"email\": \"ava.nguyen@example.com\", \"city\": \"Austin\", \"state\": \"TX\", \"country\": \"USA\", \"phone\": \"512-555-1001\"},\n",
        "    {\"customer_id\": \"CUST-002\", \"first_name\": \"Liam\", \"last_name\": \"Patel\", \"email\": \"liam.patel@example.com\", \"city\": \"Seattle\", \"state\": \"WA\", \"country\": \"USA\", \"phone\": \"206-555-1002\"},\n",
        "    {\"customer_id\": \"CUST-003\", \"first_name\": \"Sophia\", \"last_name\": \"Kim\", \"email\": \"sophia.kim@example.com\", \"city\": \"New York\", \"state\": \"NY\", \"country\": \"USA\", \"phone\": \"212-555-1003\"},\n",
        "    {\"customer_id\": \"CUST-004\", \"first_name\": \"Noah\", \"last_name\": \"Garcia\", \"email\": \"noah.garcia@example.com\", \"city\": \"Miami\", \"state\": \"FL\", \"country\": \"USA\", \"phone\": \"305-555-1004\"},\n",
        "    {\"customer_id\": \"CUST-005\", \"first_name\": \"Mia\", \"last_name\": \"Johnson\", \"email\": \"mia.johnson@example.com\", \"city\": \"Chicago\", \"state\": \"IL\", \"country\": \"USA\", \"phone\": \"773-555-1005\"},\n",
        "]\n",
        "\n",
        "products_seed = [\n",
        "    {\"product_id\": \"SKU-100\", \"product_name\": \"Wireless Mouse\", \"category\": \"Electronics\", \"sub_category\": \"Accessories\", \"list_price\": 19.99, \"supplier_code\": \"SUP-01\"},\n",
        "    {\"product_id\": \"SKU-101\", \"product_name\": \"Mechanical Keyboard\", \"category\": \"Electronics\", \"sub_category\": \"Accessories\", \"list_price\": 79.50, \"supplier_code\": \"SUP-01\"},\n",
        "    {\"product_id\": \"SKU-102\", \"product_name\": \"USB-C Charger\", \"category\": \"Electronics\", \"sub_category\": \"Power\", \"list_price\": 24.00, \"supplier_code\": \"SUP-02\"},\n",
        "    {\"product_id\": \"SKU-200\", \"product_name\": \"Water Bottle 1L\", \"category\": \"Home & Kitchen\", \"sub_category\": \"Drinkware\", \"list_price\": 12.75, \"supplier_code\": \"SUP-03\"},\n",
        "    {\"product_id\": \"SKU-300\", \"product_name\": \"Notebook Set (3)\", \"category\": \"Office\", \"sub_category\": \"Stationery\", \"list_price\": 9.99, \"supplier_code\": \"SUP-03\"},\n",
        "]\n",
        "\n",
        "spark.createDataFrame(customers_seed).write.jdbc(\n",
        "    url=JDBC_URL,\n",
        "    table=MYSQL[\"customers_table\"],\n",
        "    mode=\"overwrite\",\n",
        "    properties=mysql_props,\n",
        ")\n",
        "\n",
        "spark.createDataFrame(products_seed).write.jdbc(\n",
        "    url=JDBC_URL,\n",
        "    table=MYSQL[\"products_table\"],\n",
        "    mode=\"overwrite\",\n",
        "    properties=mysql_props,\n",
        ")\n",
        "\n",
        "print(\"Seeded MySQL tables:\", MYSQL[\"customers_table\"], \",\", MYSQL[\"products_table\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded MongoDB: northwind_purchasing . product_costs\n"
          ]
        }
      ],
      "source": [
        "product_costs_seed = [\n",
        "    {\"ProductID\": \"SKU-100\", \"UnitCost\": 12.00},\n",
        "    {\"ProductID\": \"SKU-101\", \"UnitCost\": 55.00},\n",
        "    {\"ProductID\": \"SKU-102\", \"UnitCost\": 14.00},\n",
        "    {\"ProductID\": \"SKU-200\", \"UnitCost\": 7.25},\n",
        "    {\"ProductID\": \"SKU-300\", \"UnitCost\": 4.50},\n",
        "]\n",
        "\n",
        "if MONGO[\"uri\"].startswith(\"mongodb+srv://\"):\n",
        "    mongo_client = pymongo.MongoClient(MONGO[\"uri\"], tlsCAFile=certifi.where())\n",
        "else:\n",
        "    mongo_client = pymongo.MongoClient(MONGO[\"uri\"])\n",
        "\n",
        "db = mongo_client[MONGO[\"database\"]]\n",
        "db.drop_collection(MONGO[\"collection\"])\n",
        "db[MONGO[\"collection\"]].insert_many(product_costs_seed)\n",
        "mongo_client.close()\n",
        "\n",
        "print(\"Seeded MongoDB:\", MONGO[\"database\"], \".\", MONGO[\"collection\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MySQL customers rows: 5\n",
            "MongoDB product_costs docs: 5\n"
          ]
        }
      ],
      "source": [
        "# Connectivity checks (prints counts so it's obvious which sources are being used)\n",
        "\n",
        "# 1) SQL (MySQL) check\n",
        "test_customers = (\n",
        "    spark.read.format(\"jdbc\")\n",
        "    .option(\"url\", JDBC_URL)\n",
        "    .option(\"dbtable\", MYSQL[\"customers_table\"])\n",
        "    .option(\"user\", MYSQL[\"user\"])\n",
        "    .option(\"password\", MYSQL[\"password\"])\n",
        "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\n",
        "    .load()\n",
        ")\n",
        "print(\"MySQL customers rows:\", test_customers.count())\n",
        "\n",
        "# 2) NoSQL (MongoDB Atlas/local) check\n",
        "if MONGO[\"uri\"].startswith(\"mongodb+srv://\"):\n",
        "    mongo_client = pymongo.MongoClient(MONGO[\"uri\"], tlsCAFile=certifi.where())\n",
        "else:\n",
        "    mongo_client = pymongo.MongoClient(MONGO[\"uri\"])\n",
        "\n",
        "n_docs = mongo_client[MONGO[\"database\"]][MONGO[\"collection\"]].count_documents({})\n",
        "mongo_client.close()\n",
        "print(\"MongoDB product_costs docs:\", n_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Build dimensions (Batch)\n",
        "\n",
        "**Local-source mapping (REQUIRED: SQL + NoSQL + file system)**\n",
        "- **DimDate**: file system (`data/dates.csv`)\n",
        "- **DimCustomer**: **MySQL (JDBC)** (`src_customers`) — required\n",
        "- **DimProduct**: **MySQL (JDBC)** (`src_products`) — required\n",
        "- **DimCategory**: derived from products (**3rd additional dimension**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source for customers/products: MySQL (JDBC)\n",
            "Dim rows:\n",
            "- dim_date: 10\n",
            "- dim_customer: 5\n",
            "- dim_category: 3\n",
            "- dim_product: 5\n"
          ]
        }
      ],
      "source": [
        "# Read source data (SQL + NoSQL + file system)\n",
        "\n",
        "# 1) File system (CSV): DimDate from file\n",
        "df_dates_src = spark.read.option(\"header\", True).csv(str(DATES_CSV))\n",
        "\n",
        "# 2) SQL (MySQL): customers/products via Spark JDBC\n",
        "\n",
        "def read_mysql_table(table: str):\n",
        "    return (\n",
        "        spark.read.format(\"jdbc\")\n",
        "        .option(\"url\", JDBC_URL)\n",
        "        .option(\"dbtable\", table)\n",
        "        .option(\"user\", MYSQL[\"user\"])\n",
        "        .option(\"password\", MYSQL[\"password\"])\n",
        "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\n",
        "        .load()\n",
        "    )\n",
        "\n",
        "if USE_MYSQL:\n",
        "    try:\n",
        "        df_customers_src = (\n",
        "            read_mysql_table(MYSQL[\"customers_table\"])\n",
        "            .selectExpr(\n",
        "                \"customer_id as CustomerID\",\n",
        "                \"first_name as FirstName\",\n",
        "                \"last_name as LastName\",\n",
        "                \"email as Email\",\n",
        "                \"city as City\",\n",
        "                \"state as State\",\n",
        "                \"country as Country\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        df_products_src = (\n",
        "            read_mysql_table(MYSQL[\"products_table\"])\n",
        "            .selectExpr(\n",
        "                \"product_id as ProductID\",\n",
        "                \"product_name as ProductName\",\n",
        "                \"category as Category\",\n",
        "                \"sub_category as SubCategory\",\n",
        "                \"CAST(list_price AS DOUBLE) as UnitPrice\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        print(\"Source for customers/products: MySQL (JDBC)\")\n",
        "    except Exception as e:\n",
        "        print(\"MySQL read failed — MySQL is REQUIRED for this assignment.\")\n",
        "        print(\"Error:\", str(e)[:400])\n",
        "        raise\n",
        "else:\n",
        "    raise RuntimeError(\"USE_MYSQL must be True for this assignment.\")\n",
        "\n",
        "# Basic typing\n",
        "\n",
        "dim_date = (\n",
        "    df_dates_src\n",
        "    .withColumn(\"DateKey\", F.col(\"DateKey\").cast(\"int\"))\n",
        "    .withColumn(\"Year\", F.col(\"Year\").cast(\"int\"))\n",
        "    .withColumn(\"Quarter\", F.col(\"Quarter\").cast(\"int\"))\n",
        "    .withColumn(\"Month\", F.col(\"Month\").cast(\"int\"))\n",
        "    .withColumn(\"Day\", F.col(\"Day\").cast(\"int\"))\n",
        ")\n",
        "\n",
        "dim_customer = (\n",
        "    df_customers_src\n",
        "    .select(\n",
        "        \"CustomerID\", \"FirstName\", \"LastName\", \"Email\", \"City\", \"State\", \"Country\"\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"CustomerKey\",\n",
        "        F.row_number().over(Window.orderBy(F.col(\"CustomerID\")))\n",
        "    )\n",
        ")\n",
        "\n",
        "# Category dimension derived from products\n",
        "dim_category = (\n",
        "    df_products_src\n",
        "    .select(\"Category\")\n",
        "    .dropna()\n",
        "    .dropDuplicates([\"Category\"])\n",
        "    .withColumn(\n",
        "        \"CategoryKey\",\n",
        "        F.row_number().over(Window.orderBy(F.col(\"Category\")))\n",
        "    )\n",
        ")\n",
        "\n",
        "# Product dimension (with CategoryKey FK)\n",
        "\n",
        "dim_product = (\n",
        "    df_products_src\n",
        "    .select(\"ProductID\", \"ProductName\", \"Category\", \"SubCategory\", \"UnitPrice\")\n",
        "    .withColumn(\"UnitPrice\", F.col(\"UnitPrice\").cast(\"double\"))\n",
        "    .join(dim_category, on=\"Category\", how=\"left\")\n",
        "    .withColumn(\n",
        "        \"ProductKey\",\n",
        "        F.row_number().over(Window.orderBy(F.col(\"ProductID\")))\n",
        "    )\n",
        "    .select(\"ProductKey\", \"ProductID\", \"ProductName\", \"CategoryKey\", \"Category\", \"SubCategory\", \"UnitPrice\")\n",
        ")\n",
        "\n",
        "print(\"Dim rows:\")\n",
        "print(\"- dim_date:\", dim_date.count())\n",
        "print(\"- dim_customer:\", dim_customer.count())\n",
        "print(\"- dim_category:\", dim_category.count())\n",
        "print(\"- dim_product:\", dim_product.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Batch load: Bronze → Silver → Gold\n",
        "\n",
        "This satisfies the **batch execution** requirement and creates a baseline lakehouse state before streaming increments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/19 21:25:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bronze written:\n",
            "- bronze_dim_customer: /Users/collintogher/Documents/ds-2002-proj-1/project2_lakehouse/bronze/dim_customer\n",
            "- bronze_dim_product: /Users/collintogher/Documents/ds-2002-proj-1/project2_lakehouse/bronze/dim_product\n",
            "- bronze_dim_category: /Users/collintogher/Documents/ds-2002-proj-1/project2_lakehouse/bronze/dim_category\n",
            "- bronze_dim_date: /Users/collintogher/Documents/ds-2002-proj-1/project2_lakehouse/bronze/dim_date\n",
            "- bronze_sales_batch: /Users/collintogher/Documents/ds-2002-proj-1/project2_lakehouse/bronze/sales_batch\n"
          ]
        }
      ],
      "source": [
        "def write_parquet(df, path: Path, mode: str = \"overwrite\"):\n",
        "    df.write.mode(mode).parquet(str(path))\n",
        "\n",
        "def read_parquet(path: Path):\n",
        "    return spark.read.parquet(str(path))\n",
        "\n",
        "# Where each table lives\n",
        "paths = {\n",
        "    \"bronze_dim_customer\": BRONZE_DIR / \"dim_customer\",\n",
        "    \"bronze_dim_product\":  BRONZE_DIR / \"dim_product\",\n",
        "    \"bronze_dim_category\": BRONZE_DIR / \"dim_category\",\n",
        "    \"bronze_dim_date\":     BRONZE_DIR / \"dim_date\",\n",
        "    \"bronze_sales_batch\":  BRONZE_DIR / \"sales_batch\",\n",
        "    \"bronze_sales_stream\": BRONZE_DIR / \"sales_stream\",\n",
        "\n",
        "    \"silver_fact_sales\":   SILVER_DIR / \"fact_sales\",\n",
        "\n",
        "    \"gold_dim_customer\":   GOLD_DIR / \"dim_customer\",\n",
        "    \"gold_dim_product\":    GOLD_DIR / \"dim_product\",\n",
        "    \"gold_dim_category\":   GOLD_DIR / \"dim_category\",\n",
        "    \"gold_dim_date\":       GOLD_DIR / \"dim_date\",\n",
        "    \"gold_fact_sales\":     GOLD_DIR / \"fact_sales\",\n",
        "    \"gold_fact_sales_margin\": GOLD_DIR / \"fact_sales_margin\",\n",
        "}\n",
        "\n",
        "# 3.1 Bronze: write dims + batch sales\n",
        "write_parquet(dim_customer, paths[\"bronze_dim_customer\"], mode=\"overwrite\")\n",
        "write_parquet(dim_product,  paths[\"bronze_dim_product\"],  mode=\"overwrite\")\n",
        "write_parquet(dim_category, paths[\"bronze_dim_category\"], mode=\"overwrite\")\n",
        "write_parquet(dim_date,     paths[\"bronze_dim_date\"],     mode=\"overwrite\")\n",
        "\n",
        "sales_batch = (\n",
        "    spark.read.option(\"header\", True).csv(str(SALES_CSV))\n",
        "    .withColumn(\"DateKey\", F.col(\"DateKey\").cast(\"int\"))\n",
        "    .withColumn(\"Quantity\", F.col(\"Quantity\").cast(\"double\"))\n",
        "    .withColumn(\"UnitPrice\", F.col(\"UnitPrice\").cast(\"double\"))\n",
        "    .withColumn(\"TotalAmount\", F.col(\"TotalAmount\").cast(\"double\"))\n",
        ")\n",
        "write_parquet(sales_batch, paths[\"bronze_sales_batch\"], mode=\"overwrite\")\n",
        "\n",
        "print(\"Bronze written:\")\n",
        "for k in [\n",
        "    \"bronze_dim_customer\",\"bronze_dim_product\",\"bronze_dim_category\",\"bronze_dim_date\",\"bronze_sales_batch\"\n",
        "]:\n",
        "    print(f\"- {k}: {paths[k]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1b) Incremental batch load\n",
        "\n",
        "“Batch execution + incremental load” for PySpark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bronze sales_batch rows before: 10\n",
            "Bronze sales_batch rows after:  12\n",
            "Incremental batch file: /Users/collintogher/Documents/ds-2002-proj-1/project2_incremental/sales_incremental.csv\n"
          ]
        }
      ],
      "source": [
        "before_cnt = read_parquet(paths[\"bronze_sales_batch\"]).count()\n",
        "\n",
        "inc_rows = [\n",
        "    {\n",
        "        \"DateKey\": 20250904,\n",
        "        \"CustomerID\": \"CUST-002\",\n",
        "        \"ProductID\": \"SKU-101\",\n",
        "        \"OrderID\": \"ORD-99001\",\n",
        "        \"Quantity\": 1,\n",
        "        \"UnitPrice\": 79.50,\n",
        "        \"TotalAmount\": 79.50,\n",
        "    },\n",
        "    {\n",
        "        \"DateKey\": 20250906,\n",
        "        \"CustomerID\": \"CUST-003\",\n",
        "        \"ProductID\": \"SKU-200\",\n",
        "        \"OrderID\": \"ORD-99002\",\n",
        "        \"Quantity\": 2,\n",
        "        \"UnitPrice\": 12.75,\n",
        "        \"TotalAmount\": 25.50,\n",
        "    },\n",
        "]\n",
        "\n",
        "inc_pd = pd.DataFrame(inc_rows)\n",
        "inc_path = BASE_DIR / \"project2_incremental\" / \"sales_incremental.csv\"\n",
        "inc_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "inc_pd.to_csv(inc_path, index=False)\n",
        "\n",
        "inc_df = (\n",
        "    spark.read.option(\"header\", True).csv(str(inc_path))\n",
        "    .withColumn(\"DateKey\", F.col(\"DateKey\").cast(\"int\"))\n",
        "    .withColumn(\"Quantity\", F.col(\"Quantity\").cast(\"double\"))\n",
        "    .withColumn(\"UnitPrice\", F.col(\"UnitPrice\").cast(\"double\"))\n",
        "    .withColumn(\"TotalAmount\", F.col(\"TotalAmount\").cast(\"double\"))\n",
        ")\n",
        "\n",
        "# Append to Bronze\n",
        "inc_df.write.mode(\"append\").parquet(str(paths[\"bronze_sales_batch\"]))\n",
        "\n",
        "after_cnt = read_parquet(paths[\"bronze_sales_batch\"]).count()\n",
        "print(\"Bronze sales_batch rows before:\", before_cnt)\n",
        "print(\"Bronze sales_batch rows after: \", after_cnt)\n",
        "print(\"Incremental batch file:\", inc_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source for product_costs: MongoDB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Silver fact rows: 12\n",
            "Gold margin rows: 12\n"
          ]
        }
      ],
      "source": [
        "# 3.2 Silver: conform fact to dims + compute margin\n",
        "\n",
        "# Read dims from Bronze so downstream stages are explicitly layered\n",
        "b_dim_customer = read_parquet(paths[\"bronze_dim_customer\"]).select(\"CustomerKey\",\"CustomerID\")\n",
        "b_dim_product  = read_parquet(paths[\"bronze_dim_product\"]).select(\"ProductKey\",\"ProductID\",\"CategoryKey\")\n",
        "b_dim_date     = read_parquet(paths[\"bronze_dim_date\"]).select(\"DateKey\",\"Date\")\n",
        "\n",
        "# Costs (NoSQL): MongoDB Atlas/local via pymongo \n",
        "\n",
        "def load_costs_from_mongo() -> list[dict]:\n",
        "    if MONGO[\"uri\"].startswith(\"mongodb+srv://\"):\n",
        "        client = pymongo.MongoClient(MONGO[\"uri\"], tlsCAFile=certifi.where())\n",
        "    else:\n",
        "        client = pymongo.MongoClient(MONGO[\"uri\"])\n",
        "\n",
        "    docs = list(client[MONGO[\"database\"]][MONGO[\"collection\"]].find({}))\n",
        "    client.close()\n",
        "\n",
        "    # Drop Mongo's _id field for Spark\n",
        "    for d in docs:\n",
        "        d.pop(\"_id\", None)\n",
        "    return docs\n",
        "\n",
        "costs_payload = load_costs_from_mongo()\n",
        "print(\"Source for product_costs: MongoDB\")\n",
        "\n",
        "costs_pd = pd.DataFrame(costs_payload)\n",
        "dim_costs = spark.createDataFrame(costs_pd).withColumn(\"UnitCost\", F.col(\"UnitCost\").cast(\"double\"))\n",
        "\n",
        "# Fact source for batch baseline\n",
        "b_sales = read_parquet(paths[\"bronze_sales_batch\"]).select(\n",
        "    \"DateKey\",\"CustomerID\",\"ProductID\",\"OrderID\",\"Quantity\",\"UnitPrice\",\"TotalAmount\"\n",
        ")\n",
        "\n",
        "fact_silver = (\n",
        "    b_sales\n",
        "    .join(b_dim_customer, on=\"CustomerID\", how=\"left\")\n",
        "    .join(b_dim_product,  on=\"ProductID\",  how=\"left\")\n",
        "    .withColumn(\n",
        "        \"SalesKey\",\n",
        "        F.row_number().over(Window.orderBy(F.col(\"OrderID\"), F.col(\"CustomerID\"), F.col(\"ProductID\")))\n",
        "    )\n",
        "    .select(\n",
        "        \"SalesKey\",\n",
        "        \"DateKey\",\n",
        "        \"CustomerKey\",\n",
        "        \"ProductKey\",\n",
        "        \"OrderID\",\n",
        "        \"Quantity\",\n",
        "        \"UnitPrice\",\n",
        "        \"TotalAmount\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Enrich with margin\n",
        "fact_silver_margin = (\n",
        "    fact_silver\n",
        "    .join(b_dim_product.select(\"ProductKey\",\"ProductID\"), on=\"ProductKey\", how=\"left\")\n",
        "    .join(dim_costs, on=\"ProductID\", how=\"left\")\n",
        "    .withColumn(\"UnitCost\", F.coalesce(F.col(\"UnitCost\"), F.lit(0.0)))\n",
        "    .withColumn(\"GrossMarginAmount\", F.col(\"TotalAmount\") - (F.col(\"UnitCost\") * F.col(\"Quantity\")))\n",
        "    .select(\"SalesKey\",\"GrossMarginAmount\")\n",
        ")\n",
        "\n",
        "write_parquet(fact_silver, paths[\"silver_fact_sales\"], mode=\"overwrite\")\n",
        "write_parquet(fact_silver_margin, paths[\"gold_fact_sales_margin\"], mode=\"overwrite\")\n",
        "\n",
        "print(\"Silver fact rows:\", fact_silver.count())\n",
        "print(\"Gold margin rows:\", fact_silver_margin.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gold tables written.\n"
          ]
        }
      ],
      "source": [
        "# 3.3 Gold: publish star schema tables\n",
        "\n",
        "write_parquet(read_parquet(paths[\"bronze_dim_customer\"]), paths[\"gold_dim_customer\"], mode=\"overwrite\")\n",
        "write_parquet(read_parquet(paths[\"bronze_dim_product\"]),  paths[\"gold_dim_product\"],  mode=\"overwrite\")\n",
        "write_parquet(read_parquet(paths[\"bronze_dim_category\"]), paths[\"gold_dim_category\"], mode=\"overwrite\")\n",
        "write_parquet(read_parquet(paths[\"bronze_dim_date\"]),     paths[\"gold_dim_date\"],     mode=\"overwrite\")\n",
        "\n",
        "# Gold fact = silver fact (serving layer)\n",
        "write_parquet(read_parquet(paths[\"silver_fact_sales\"]), paths[\"gold_fact_sales\"], mode=\"overwrite\")\n",
        "\n",
        "print(\"Gold tables written.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Streaming requirement: 3 mini-batches (local file stream)\n",
        "\n",
        "**Requirement mapping**\n",
        "- Mimics Databricks AutoLoader using **Structured Streaming** reading from a local folder.\n",
        "- Uses **3 intervals** by dropping 3 JSON files into `project2_stream/incoming/`.\n",
        "- Uses **checkpointing** so each run only processes new files.\n",
        "\n",
        "Implementation approach (simple + deterministic):\n",
        "- Split `data/sales.csv` into 3 parts.\n",
        "- For each part: write a JSON file into `incoming/`, run a streaming query with `trigger(once=True)`, and stop.\n",
        "- Checkpoint state guarantees exactly-once file processing across the 3 intervals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4, 4, 2]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4.1 Create 3 streaming JSON drops from the same Project-1 sales.csv\n",
        "\n",
        "sales_pd = pd.read_csv(SALES_CSV)\n",
        "\n",
        "# Split into 3 approximately-equal parts (deterministic order)\n",
        "sales_pd = sales_pd.sort_values([\"DateKey\",\"OrderID\",\"CustomerID\",\"ProductID\"]).reset_index(drop=True)\n",
        "parts = [\n",
        "    sales_pd.iloc[0:4].copy(),\n",
        "    sales_pd.iloc[4:8].copy(),\n",
        "    sales_pd.iloc[8:].copy(),\n",
        "]\n",
        "\n",
        "def write_drop(df_part: pd.DataFrame, drop_idx: int) -> Path:\n",
        "    out_path = INCOMING_DIR / f\"sales_drop_{drop_idx:02d}.json\"\n",
        "    records = df_part.to_dict(orient=\"records\")\n",
        "\n",
        "    with open(out_path, \"w\") as f:\n",
        "        for rec in records:\n",
        "            f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "    return out_path\n",
        "\n",
        "[len(p) for p in parts]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/19 21:25:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After interval 1, bronze/sales_stream rows: 3\n",
            "After interval 2, bronze/sales_stream rows: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/19 21:25:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "25/12/19 21:25:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After interval 3, bronze/sales_stream rows: 3\n",
            "Drops written: sales_drop_01.json sales_drop_02.json sales_drop_03.json\n"
          ]
        }
      ],
      "source": [
        "# 4.2 Streaming ingestion (Bronze): read JSON drops from incoming/ and append to bronze/sales_stream\n",
        "# This is the local equivalent of Databricks AutoLoader ingestion.\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "sales_schema = StructType([\n",
        "    StructField(\"DateKey\", IntegerType(), True),\n",
        "    StructField(\"CustomerID\", StringType(), True),\n",
        "    StructField(\"ProductID\", StringType(), True),\n",
        "    StructField(\"OrderID\", StringType(), True),\n",
        "    StructField(\"Quantity\", DoubleType(), True),\n",
        "    StructField(\"UnitPrice\", DoubleType(), True),\n",
        "    StructField(\"TotalAmount\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "bronze_sales_stream_path = paths[\"bronze_sales_stream\"]\n",
        "\n",
        "def run_stream_once() -> None:\n",
        "    stream_df = (\n",
        "        spark.readStream\n",
        "        .schema(sales_schema)\n",
        "        # Tolerate either NDJSON (preferred) or multi-line JSON arrays\n",
        "        .option(\"multiLine\", \"true\")\n",
        "        .json(str(INCOMING_DIR))\n",
        "        .withColumn(\"ingest_ts\", F.current_timestamp())\n",
        "    )\n",
        "\n",
        "    q = (\n",
        "        stream_df.writeStream\n",
        "        .format(\"parquet\")\n",
        "        .outputMode(\"append\")\n",
        "        .option(\"checkpointLocation\", str(CHECKPOINT_DIR))\n",
        "        .trigger(once=True)\n",
        "        .start(str(bronze_sales_stream_path))\n",
        "    )\n",
        "    q.awaitTermination()\n",
        "\n",
        "# Interval 1\n",
        "p1 = write_drop(parts[0], 1)\n",
        "run_stream_once()\n",
        "print(\"After interval 1, bronze/sales_stream rows:\", read_parquet(bronze_sales_stream_path).count())\n",
        "\n",
        "# Interval 2\n",
        "p2 = write_drop(parts[1], 2)\n",
        "run_stream_once()\n",
        "print(\"After interval 2, bronze/sales_stream rows:\", read_parquet(bronze_sales_stream_path).count())\n",
        "\n",
        "# Interval 3\n",
        "p3 = write_drop(parts[2], 3)\n",
        "run_stream_once()\n",
        "print(\"After interval 3, bronze/sales_stream rows:\", read_parquet(bronze_sales_stream_path).count())\n",
        "\n",
        "print(\"Drops written:\", p1.name, p2.name, p3.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bronze stream row count: 3\n",
            "+--------+----------+---------+---------+--------+---------+-----------+-----------------------+\n",
            "|DateKey |CustomerID|ProductID|OrderID  |Quantity|UnitPrice|TotalAmount|ingest_ts              |\n",
            "+--------+----------+---------+---------+--------+---------+-----------+-----------------------+\n",
            "|20250901|CUST-001  |SKU-100  |ORD-90001|2.0     |19.99    |39.98      |2025-12-19 14:19:25.569|\n",
            "|20250905|CUST-005  |SKU-300  |ORD-90005|4.0     |9.99     |39.96      |2025-12-19 14:19:26.089|\n",
            "|20250910|CUST-004  |SKU-300  |ORD-90009|2.0     |9.99     |19.98      |2025-12-19 14:19:26.451|\n",
            "+--------+----------+---------+---------+--------+---------+-----------+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sanity check: if this is 0, downstream Silver/Gold/queries will be empty.\n",
        "\n",
        "bronze_cnt = read_parquet(paths[\"bronze_sales_stream\"]).count() if paths[\"bronze_sales_stream\"].exists() else 0\n",
        "print(\"Bronze stream row count:\", bronze_cnt)\n",
        "\n",
        "if bronze_cnt > 0:\n",
        "    read_parquet(paths[\"bronze_sales_stream\"]).orderBy(\"ingest_ts\").show(20, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Silver: join streaming facts to static dimensions (required)\n",
        "\n",
        "This section satisfies the Project 2 requirement to **illustrate relationships** between near real-time fact data and static reference data by joining at the Silver layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "25/12/19 21:25:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Silver (from stream) fact rows: 3\n"
          ]
        }
      ],
      "source": [
        "# Build a conformed fact table from the streaming bronze output\n",
        "\n",
        "stream_bronze = read_parquet(paths[\"bronze_sales_stream\"]).select(\n",
        "    \"DateKey\",\"CustomerID\",\"ProductID\",\"OrderID\",\"Quantity\",\"UnitPrice\",\"TotalAmount\",\"ingest_ts\"\n",
        ")\n",
        "\n",
        "# Join to static dims (from Bronze) to get surrogate keys\n",
        "conformed_stream = (\n",
        "    stream_bronze\n",
        "    .join(b_dim_customer, on=\"CustomerID\", how=\"left\")\n",
        "    .join(b_dim_product,  on=\"ProductID\",  how=\"left\")\n",
        ")\n",
        "\n",
        "# Create a deterministic SalesKey for this streaming dataset\n",
        "conformed_stream = conformed_stream.withColumn(\n",
        "    \"SalesKey\",\n",
        "    F.row_number().over(Window.orderBy(F.col(\"ingest_ts\"), F.col(\"OrderID\"), F.col(\"CustomerID\"), F.col(\"ProductID\")))\n",
        ")\n",
        "\n",
        "silver_stream_fact = conformed_stream.select(\n",
        "    \"SalesKey\",\n",
        "    \"DateKey\",\n",
        "    \"CustomerKey\",\n",
        "    \"ProductKey\",\n",
        "    \"OrderID\",\n",
        "    \"Quantity\",\n",
        "    \"UnitPrice\",\n",
        "    \"TotalAmount\",\n",
        "    \"ingest_ts\"\n",
        ")\n",
        "\n",
        "# Overwrite Silver fact with the full conformed stream result (simple replay semantics)\n",
        "write_parquet(silver_stream_fact, paths[\"silver_fact_sales\"], mode=\"overwrite\")\n",
        "\n",
        "print(\"Silver (from stream) fact rows:\", silver_stream_fact.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5b) Publish Gold fact from streaming + recompute margin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gold fact rows: 3\n",
            "Gold margin rows: 3\n"
          ]
        }
      ],
      "source": [
        "# Overwrite Gold fact with the conformed streaming fact\n",
        "write_parquet(read_parquet(paths[\"silver_fact_sales\"]), paths[\"gold_fact_sales\"], mode=\"overwrite\")\n",
        "\n",
        "# Recompute margin aligned to streaming SalesKey\n",
        "# Join ProductKey -> ProductID -> UnitCost, then compute GrossMarginAmount.\n",
        "\n",
        "gold_fact = read_parquet(paths[\"gold_fact_sales\"]).select(\"SalesKey\",\"ProductKey\",\"Quantity\",\"TotalAmount\")\n",
        "prod_key_to_id = read_parquet(paths[\"gold_dim_product\"]).select(\"ProductKey\",\"ProductID\")\n",
        "\n",
        "fact_for_margin = gold_fact.join(prod_key_to_id, on=\"ProductKey\", how=\"left\")\n",
        "\n",
        "fact_margin_stream = (\n",
        "    fact_for_margin\n",
        "    .join(dim_costs, on=\"ProductID\", how=\"left\")\n",
        "    .withColumn(\"UnitCost\", F.coalesce(F.col(\"UnitCost\"), F.lit(0.0)))\n",
        "    .withColumn(\"GrossMarginAmount\", F.col(\"TotalAmount\") - (F.col(\"UnitCost\") * F.col(\"Quantity\")))\n",
        "    .select(\"SalesKey\",\"GrossMarginAmount\")\n",
        ")\n",
        "\n",
        "write_parquet(fact_margin_stream, paths[\"gold_fact_sales_margin\"], mode=\"overwrite\")\n",
        "\n",
        "print(\"Gold fact rows:\", read_parquet(paths[\"gold_fact_sales\"]).count())\n",
        "print(\"Gold margin rows:\", read_parquet(paths[\"gold_fact_sales_margin\"]).count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Source from MySQL and/or MongoDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dim_date': 10, 'dim_product': 5, 'dim_category': 3, 'fact_sales': 3, 'fact_sales_margin': 3}\n",
            "fact + product: 3\n",
            "fact + product + category: 3\n",
            "fact + margin: 3\n",
            "fact + product + category + margin: 3\n"
          ]
        }
      ],
      "source": [
        "def ensure_gold_views() -> None:\n",
        "    view_to_gold_path_key = {\n",
        "        \"dim_date\": \"gold_dim_date\",\n",
        "        \"dim_customer\": \"gold_dim_customer\",\n",
        "        \"dim_product\": \"gold_dim_product\",\n",
        "        \"dim_category\": \"gold_dim_category\",\n",
        "        \"fact_sales\": \"gold_fact_sales\",\n",
        "        \"fact_sales_margin\": \"gold_fact_sales_margin\",\n",
        "    }\n",
        "\n",
        "    for view_name, path_key in view_to_gold_path_key.items():\n",
        "        if not spark.catalog.tableExists(view_name):\n",
        "            read_parquet(paths[path_key]).createOrReplaceTempView(view_name)\n",
        "\n",
        "ensure_gold_views()\n",
        "\n",
        "checks = {\n",
        "    \"dim_date\": spark.table(\"dim_date\").count(),\n",
        "    \"dim_product\": spark.table(\"dim_product\").count(),\n",
        "    \"dim_category\": spark.table(\"dim_category\").count(),\n",
        "    \"fact_sales\": spark.table(\"fact_sales\").count(),\n",
        "    \"fact_sales_margin\": spark.table(\"fact_sales_margin\").count(),\n",
        "}\n",
        "print(checks)\n",
        "\n",
        "# How many fact rows survive each join?\n",
        "print(\"fact + product:\", spark.sql(\"\"\"\n",
        "SELECT COUNT(*) AS cnt\n",
        "FROM fact_sales f\n",
        "JOIN dim_product p ON f.ProductKey = p.ProductKey\n",
        "\"\"\").collect()[0][0])\n",
        "\n",
        "print(\"fact + product + category:\", spark.sql(\"\"\"\n",
        "SELECT COUNT(*) AS cnt\n",
        "FROM fact_sales f\n",
        "JOIN dim_product p  ON f.ProductKey = p.ProductKey\n",
        "JOIN dim_category c ON p.CategoryKey = c.CategoryKey\n",
        "\"\"\").collect()[0][0])\n",
        "\n",
        "print(\"fact + margin:\", spark.sql(\"\"\"\n",
        "SELECT COUNT(*) AS cnt\n",
        "FROM fact_sales f\n",
        "JOIN fact_sales_margin m ON f.SalesKey = m.SalesKey\n",
        "\"\"\").collect()[0][0])\n",
        "\n",
        "print(\"fact + product + category + margin:\", spark.sql(\"\"\"\n",
        "SELECT COUNT(*) AS cnt\n",
        "FROM fact_sales f\n",
        "JOIN dim_product p        ON f.ProductKey = p.ProductKey\n",
        "JOIN dim_category c       ON p.CategoryKey = c.CategoryKey\n",
        "JOIN fact_sales_margin m  ON f.SalesKey = m.SalesKey\n",
        "\"\"\").collect()[0][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Gold analytics + queries (business value)\n",
        "\n",
        "All queries below read from the **Gold** star schema tables and demonstrate aggregation across **Fact + 2+ dimensions**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Views created: dim_date, dim_customer, dim_product, dim_category, fact_sales, fact_sales_margin\n"
          ]
        }
      ],
      "source": [
        "# Register Gold tables as temp views for Spark SQL queries\n",
        "\n",
        "g_dim_date = read_parquet(paths[\"gold_dim_date\"])\n",
        "g_dim_customer = read_parquet(paths[\"gold_dim_customer\"])\n",
        "g_dim_product = read_parquet(paths[\"gold_dim_product\"])\n",
        "g_dim_category = read_parquet(paths[\"gold_dim_category\"])\n",
        "\n",
        "g_fact_sales = read_parquet(paths[\"gold_fact_sales\"])  # from Silver (stream)\n",
        "g_fact_margin = read_parquet(paths[\"gold_fact_sales_margin\"])  # computed earlier\n",
        "\n",
        "\n",
        "# Publish views\n",
        "for name, df in {\n",
        "    \"dim_date\": g_dim_date,\n",
        "    \"dim_customer\": g_dim_customer,\n",
        "    \"dim_product\": g_dim_product,\n",
        "    \"dim_category\": g_dim_category,\n",
        "    \"fact_sales\": g_fact_sales,\n",
        "    \"fact_sales_margin\": g_fact_margin,\n",
        "}.items():\n",
        "    df.createOrReplaceTempView(name)\n",
        "\n",
        "print(\"Views created:\", \"dim_date, dim_customer, dim_product, dim_category, fact_sales, fact_sales_margin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+-----------+\n",
            "|Date      |Revenue|GrossMargin|\n",
            "+----------+-------+-----------+\n",
            "|2025-09-01|39.98  |15.98      |\n",
            "|2025-09-05|39.96  |21.96      |\n",
            "|2025-09-10|19.98  |10.98      |\n",
            "+----------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query 1: Daily revenue + gross margin (Fact + DimDate + Margin)\n",
        "\n",
        "q1 = \"\"\"\n",
        "SELECT\n",
        "  d.Date,\n",
        "  ROUND(SUM(f.TotalAmount), 2) AS Revenue,\n",
        "  ROUND(SUM(m.GrossMarginAmount), 2) AS GrossMargin\n",
        "FROM fact_sales f\n",
        "JOIN dim_date d ON f.DateKey = d.DateKey\n",
        "JOIN fact_sales_margin m ON f.SalesKey = m.SalesKey\n",
        "GROUP BY d.Date\n",
        "ORDER BY d.Date\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(q1).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+-------+-----------+\n",
            "|Date      |Category   |Revenue|GrossMargin|\n",
            "+----------+-----------+-------+-----------+\n",
            "|2025-09-01|Electronics|39.98  |15.98      |\n",
            "|2025-09-05|Office     |39.96  |21.96      |\n",
            "|2025-09-10|Office     |19.98  |10.98      |\n",
            "+----------+-----------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query 2: Revenue + margin by Date + Category (Fact + DimDate + DimProduct + DimCategory)\n",
        "\n",
        "q2 = \"\"\"\n",
        "SELECT\n",
        "  d.Date,\n",
        "  c.Category,\n",
        "  ROUND(SUM(f.TotalAmount), 2) AS Revenue,\n",
        "  ROUND(SUM(m.GrossMarginAmount), 2) AS GrossMargin\n",
        "FROM fact_sales f\n",
        "JOIN dim_date d      ON f.DateKey = d.DateKey\n",
        "JOIN dim_product p   ON f.ProductKey = p.ProductKey\n",
        "JOIN dim_category c  ON p.CategoryKey = c.CategoryKey\n",
        "JOIN fact_sales_margin m ON f.SalesKey = m.SalesKey\n",
        "GROUP BY d.Date, c.Category\n",
        "ORDER BY d.Date, c.Category\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(q2).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query 3 — Top customers by margin\n",
            "+----------+------------+-------+-----------+----------+\n",
            "|CustomerID|CustomerName|Revenue|GrossMargin|OrderCount|\n",
            "+----------+------------+-------+-----------+----------+\n",
            "|CUST-005  |Mia Johnson |39.96  |21.96      |1         |\n",
            "|CUST-001  |Ava Nguyen  |39.98  |15.98      |1         |\n",
            "|CUST-004  |Noah Garcia |19.98  |10.98      |1         |\n",
            "+----------+------------+-------+-----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query 3: Top customers by gross margin\n",
        "q3 = \"\"\"\n",
        "SELECT\n",
        "  c.CustomerID,\n",
        "  CONCAT(c.FirstName, ' ', c.LastName) AS CustomerName,\n",
        "  ROUND(SUM(f.TotalAmount), 2) AS Revenue,\n",
        "  ROUND(SUM(m.GrossMarginAmount), 2) AS GrossMargin,\n",
        "  COUNT(DISTINCT f.OrderID) AS OrderCount\n",
        "FROM fact_sales f\n",
        "JOIN dim_customer c       ON f.CustomerKey = c.CustomerKey\n",
        "JOIN fact_sales_margin m  ON f.SalesKey = m.SalesKey\n",
        "GROUP BY c.CustomerID, CustomerName\n",
        "ORDER BY GrossMargin DESC, Revenue DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"Query 3 — Top customers by margin\")\n",
        "spark.sql(q3).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query 4 — Product leaderboard\n",
            "+---------+----------------+-----------+---------+-------+-----------+\n",
            "|ProductID|ProductName     |Category   |UnitsSold|Revenue|GrossMargin|\n",
            "+---------+----------------+-----------+---------+-------+-----------+\n",
            "|SKU-300  |Notebook Set (3)|Office     |6.0      |59.94  |32.94      |\n",
            "|SKU-100  |Wireless Mouse  |Electronics|2.0      |39.98  |15.98      |\n",
            "+---------+----------------+-----------+---------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query 4: Product leaderboard (units, revenue, margin)\n",
        "q4 = \"\"\"\n",
        "SELECT\n",
        "  p.ProductID,\n",
        "  p.ProductName,\n",
        "  c.Category,\n",
        "  ROUND(SUM(f.Quantity), 2) AS UnitsSold,\n",
        "  ROUND(SUM(f.TotalAmount), 2) AS Revenue,\n",
        "  ROUND(SUM(m.GrossMarginAmount), 2) AS GrossMargin\n",
        "FROM fact_sales f\n",
        "JOIN dim_product p        ON f.ProductKey = p.ProductKey\n",
        "JOIN dim_category c       ON p.CategoryKey = c.CategoryKey\n",
        "JOIN fact_sales_margin m  ON f.SalesKey = m.SalesKey\n",
        "GROUP BY p.ProductID, p.ProductName, c.Category\n",
        "ORDER BY GrossMargin DESC, Revenue DESC, UnitsSold DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"Query 4 — Product leaderboard\")\n",
        "spark.sql(q4).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query 5 — AOV/AOM by weekday\n",
            "+---------+-------------+--------------+------+\n",
            "|DayOfWeek|AvgOrderValue|AvgOrderMargin|Orders|\n",
            "+---------+-------------+--------------+------+\n",
            "|Friday   |39.96        |21.96         |1     |\n",
            "|Monday   |39.98        |15.98         |1     |\n",
            "|Wednesday|19.98        |10.98         |1     |\n",
            "+---------+-------------+--------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query 5: Average order value (AOV) + average order margin (AOM) by weekday\n",
        "q5 = \"\"\"\n",
        "SELECT\n",
        "  d.DayOfWeek,\n",
        "  ROUND(SUM(f.TotalAmount) / COUNT(DISTINCT f.OrderID), 2) AS AvgOrderValue,\n",
        "  ROUND(SUM(m.GrossMarginAmount) / COUNT(DISTINCT f.OrderID), 2) AS AvgOrderMargin,\n",
        "  COUNT(DISTINCT f.OrderID) AS Orders\n",
        "FROM fact_sales f\n",
        "JOIN dim_date d          ON f.DateKey = d.DateKey\n",
        "JOIN fact_sales_margin m ON f.SalesKey = m.SalesKey\n",
        "GROUP BY d.DayOfWeek\n",
        "ORDER BY Orders DESC, AvgOrderMargin DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"Query 5 — AOV/AOM by weekday\")\n",
        "spark.sql(q5).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query 6 — Monthly rollup\n",
            "+----+-----+-------+-----------+------+\n",
            "|Year|Month|Revenue|GrossMargin|Orders|\n",
            "+----+-----+-------+-----------+------+\n",
            "|2025|9    |99.92  |48.92      |3     |\n",
            "+----+-----+-------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query 6: Monthly revenue + margin rollup\n",
        "q6 = \"\"\"\n",
        "SELECT\n",
        "  d.Year,\n",
        "  d.Month,\n",
        "  ROUND(SUM(f.TotalAmount), 2) AS Revenue,\n",
        "  ROUND(SUM(m.GrossMarginAmount), 2) AS GrossMargin,\n",
        "  COUNT(DISTINCT f.OrderID) AS Orders\n",
        "FROM fact_sales f\n",
        "JOIN dim_date d          ON f.DateKey = d.DateKey\n",
        "JOIN fact_sales_margin m ON f.SalesKey = m.SalesKey\n",
        "GROUP BY d.Year, d.Month\n",
        "ORDER BY d.Year, d.Month\n",
        "\"\"\"\n",
        "\n",
        "print(\"Query 6 — Monthly rollup\")\n",
        "spark.sql(q6).show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
